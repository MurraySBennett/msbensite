[
  {
    "id": "censoring-accumulator-models",
    "title": "Censoring Accumulator Models",
    "category": "Cognitive Modeling & Decision Science",
    "summary": "Examining censoring methods for response time data in evidence accumulation models (EAMs) to improve the accuracy and interpretability of cognitive insights. This project delves into how different approaches to handling response time outliers impact model parameter estimation and theoretical conclusions.",
    "image": "https://placehold.co/600x400/ADD8E6/4682B4?text=Censoring+EAMs",
    "background": [
      "Evidence Accumulation Models (EAMs) are powerful tools for understanding cognitive processes underlying decision-making, particularly when analyzing response time and accuracy data. However, real-world response time data often contains 'outliers' or 'censored' observations (e.g., very fast guesses, very slow responses due to distraction). The method used to handle these observations can significantly influence model parameter estimates and, consequently, the psychological interpretations drawn from the model. This project systematically investigates various censoring techniques and their effects on model fit and parameter recovery."
    ],
    "analyses": [
      "Simulations comparing parameter recovery under different censoring methods (e.g., fixed cutoff, quantile-based, likelihood-based).",
      "Application of optimal censoring methods to empirical datasets to demonstrate improved model fit and more robust parameter estimates.",
      "Comparison of model-based inferences (e.g., drift rate, boundary separation) across different censoring strategies."
    ],
    "outcomes": [""],
    "interactive_demo_link": "",
    "experiment_demo_link": "",
    "osf_link": ""
  },
  {
    "id": "confidnet",
    "title": "ConfidNet: Confidence Estimation for Melanoma Classification",
    "category": "Human-AI Collaboration",
    "summary": "Generating and evaluating confidence estimates for melanoma classification from AI models, focusing on how these estimates can foster more robust and transparent human-AI collaboration in medical diagnosis.",
    "image": "https://placehold.co/600x400/FF69B4/FFF?text=ConfidNet",
    "background": [
      "In critical domains like medical diagnosis, AI systems are increasingly used to assist human experts. However, for effective collaboration, humans need to understand not just an AI's prediction, but also its confidence in that prediction. This project develops and evaluates novel methods for generating reliable confidence estimates from Convolutional Neural Networks (CNNs) for melanoma classification. The goal is to provide human dermatologists with actionable insights into the AI's internal state, enabling better-informed decisions and appropriate trust calibration."
    ],
    "analyses": [
      "Development of post-hoc calibration techniques for CNN confidence scores.",
      "Evaluation of confidence metrics (e.g., Expected Calibration Error, Brier Score) on diverse melanoma datasets.",
      "Analysis of human trust and reliance on AI predictions based on provided confidence levels."
    ],
    "outcomes": [""],
    "interactive_demo_link": "interactive-confidnet-analysis.html",
    "experiment_demo_link": "",
    "osf_link": ""
  },
  {
    "id": "discrete-choice-rating",
    "title": "Discrete Choice vs. Rating Scale Decision Elicitation",
    "category": "Cognitive Modeling & Decision Science",
    "summary": "Comparing discrete choice and rating scale decision elicitation methods using ground-truth perceptual stimuli, with implications for understanding human perception and decision-making processes.",
    "image": "assets/images/project-icons/dc-rs-demo.png",
    "background": [
      "In psychological research, decisions are often collected using either discrete choices (e.g., 'yes' or 'no') or rating scales (e.g., '1-7 confidence'). This project investigates whether these different elicitation methods yield equivalent insights into underlying perceptual and decision processes, especially when ground-truth stimuli are available. Using carefully controlled perceptual tasks, we compare the sensitivity, bias, and consistency of data obtained from discrete choice versus rating scale paradigms, informing best practices for experimental design."
    ],
    "analyses": [
      "Signal Detection Theory (SDT) analysis comparing sensitivity (d') and bias (c) across elicitation methods.",
      "Cognitive modeling (e.g., diffusion model) applied to both discrete and rating data to assess parameter consistency.",
      "Analysis of response time distributions for both decision types."
    ],
    "outcomes": [
      "Preliminary findings suggest that while both methods can capture similar underlying perceptual sensitivities, rating scales may provide richer information about confidence and uncertainty. However, discrete choices tend to be more robust to certain types of noise. These insights have important implications for experimental design in cognitive psychology and related fields."
    ],
    "interactive_demo_link": "",
    "experiment_demo_link": "experiment-demo-viewer.html?demo=dc-rs",
    "osf_link": ""
  },
  {
    "id": "dutch-auction",
    "title": "Competitive Decision Making in Dutch Auctions",
    "category": "Cognitive Modeling & Decision Science",
    "summary": "Examining competitive decision making in Dutch auctions among groups through the lens of Prospect Theory and other cognitive models, focusing on factors influencing group performance and individual bidding strategies.",
    "image": "assets/images/project-icons/dutch-auction-demo.png",
    "background": [
      "Dutch auctions, where prices decrease until a bidder claims the item, offer a unique setting to study competitive decision-making under uncertainty. This project investigates how individuals and groups behave in these auctions, particularly focusing on the role of risk perception and framing effects as described by Prospect Theory. We analyze bidding patterns, response times, and group dynamics to understand the cognitive mechanisms driving decisions in high-stakes, competitive environments."
    ],
    "analyses": [
      "Modeling individual bidding behavior using Prospect Theory parameters.",
      "Analysis of group coordination and emergent strategies.",
      "Comparison of individual vs. group performance efficiency in various auction conditions."
    ],
    "outcomes": [
      "Initial results indicate that individuals often deviate from optimal bidding strategies due to loss aversion and risk-seeking behavior in certain contexts. Groups tend to perform better than individuals, likely due to shared information and diverse strategies, but are still influenced by cognitive biases. These findings contribute to our understanding of decision-making in competitive environments and have practical implications for auction design."
    ],
    "interactive_demo_link": "",
    "experiment_demo_link": "experiment-demo-viewer.html?demo=dutch-auction",
    "osf_link": ""
  },
  {
    "id": "noisy-grt",
    "title": "Noisy General Recognition Theory (GRT)",
    "category": "Cognitive Modeling & Decision Science",
    "summary": "Developing General Recognition Theory (GRT) models that explicitly incorporate noise and dimensional collapse, providing a more nuanced understanding of perceptual decision-making, particularly relevant for complex stimuli like medical images.",
    "image": "https://placehold.co/600x400/ADD8E6/4682B4?text=Noisy+GRT",
    "background": [
      "General Recognition Theory (GRT) is a powerful framework for modeling multidimensional perceptual decision-making. However, real-world perception often involves inherent noise and the 'collapse' of information across dimensions (i.g., ignoring irrelevant features). This project extends traditional GRT models to account for these complexities, leading to more accurate descriptions of human perceptual processes, especially in tasks involving high-dimensional stimuli like dermatological images where subtle features are critical but can be noisy or correlated."
    ],
    "analyses": [
      "Mathematical derivation and implementation of GRT models with noise and dimensional collapse parameters.",
      "Goodness-of-fit analysis comparing noisy GRT to traditional GRT on simulated and empirical data.",
      "Interpretation of noise and dimensional collapse parameters in the context of perceptual expertise."
    ],
    "outcomes": [""],
    "interactive_demo_link": "",
    "experiment_demo_link": "",
    "osf_link": ""
  },
  {
    "id": "wheel-of-fortune",
    "title": "Wheel of Fortune: Unbiased Mental Representations",
    "category": "Cognitive Modeling & Decision Science",
    "summary": "Examining unbiased mental representations of symbolic numerals, using a 'Wheel of Fortune' paradigm to investigate how humans process and represent numerical information without explicit biases.",
    "image": "assets/images/project-icons/wheel-of-fortune-demo.png",
    "background": [
      "Numerical cognition research often explores how humans represent and use numbers. This project utilizes a modified 'Wheel of Fortune' task to probe the underlying mental representations of symbolic numerals (e.g., digits). By manipulating the task structure, we aim to uncover whether these representations are inherently unbiased or if they are influenced by contextual factors and cognitive heuristics. The findings contribute to our understanding of fundamental numerical processing and decision-making."
    ],
    "analyses": [
      "Analysis of response distributions and biases in numerical estimation.",
      "Modeling of underlying mental number line representations.",
      "Comparison of performance across different numerical ranges and presentation formats."
    ],
    "outcomes": [""],
    "interactive_demo_link": "",
    "experiment_demo_link": "experiment-demo-viewer.html?demo=wheel-of-fortune",
    "osf_link": ""
  },
  {
    "id": "melanoma-features",
    "title": "ABC Feature Rating for Melanoma Identification",
    "category": "Perception & Applied Vision (Melanoma Identification)",
    "summary": "Developing a continuous quantitative measure of the perceptual strength of shape asymmetry, border irregularity, and color variance (ABC features) for melanoma identification, crucial for both human and AI diagnostic accuracy.",
    "image": "assets/images/project-icons/melanoma-features-demo.png",
    "background": [
      "The ABCDE rule (Asymmetry, Border irregularity, Color variance, Diameter, Evolving) is a widely used mnemonic for melanoma detection. This project focuses on the 'ABC' features, aiming to develop a continuous, quantitative method for rating their perceptual strength. This goes beyond simple binary presence/absence and provides a more fine-grained measure that can be used to train human experts, evaluate AI systems, and improve the consistency of dermatological assessments."
    ],
    "analyses": [
      "Development and validation of a psychophysical rating scale for ABC features.",
      "Correlation analysis between subjective ABC ratings and objective image metrics.",
      "Assessment of inter-rater reliability for the continuous ABC rating system."
    ],
    "outcomes": [""],
    "interactive_demo_link": "experiment-demo-viewer.html?demo=mel-features",
    "experiment_demo_link": "",
    "osf_link": ""
  },
  {
    "id": "abc-feature-interactions",
    "title": "ABC Feature Interactions in Melanoma Identification",
    "category": "Perception & Applied Vision (Melanoma Identification)",
    "summary": "Evaluating perceptual interactions among ABC features in melanoma identification, investigating how the combination of asymmetry, border irregularity, and color variance influences diagnostic judgments.",
    "image": "https://placehold.co/600x400/ADD8E6/4682B4?text=ABC+Interactions",
    "background": [
      "While individual ABC features are important for melanoma detection, real-world lesions present a complex interplay of these characteristics. This project specifically examines how human perception integrates and weighs these features when they appear in combination. Understanding these perceptual interactions is critical for improving diagnostic training, developing more effective decision aids, and creating AI systems that align with human perceptual expertise."
    ],
    "analyses": [
      "Experimental design to manipulate combinations of ABC features.",
      "Analysis of human judgments (e.g., malignancy ratings, confidence) in response to interacting features.",
      "Cognitive modeling (e.g., General Recognition Theory) to quantify the independence or integration of feature processing."
    ],
    "outcomes": [""],
    "interactive_demo_link": "interactive-abc-interactions.html",
    "experiment_demo_link": "",
    "osf_link": ""
  },
  {
    "id": "melnet-training",
    "title": "MelNet: Training Human Perceptual Expertise",
    "category": "Human-AI Collaboration",
    "summary": "Training human perceptual expertise using CDNN (Convolutional Deep Neural Network) activations, aiming to transfer AI insights to human learners for improved melanoma identification.",
    "image": "https://placehold.co/600x400/FF69B4/FFF?text=MelNet+Training",
    "background": [
      "While AI excels at complex pattern recognition, transferring its learned 'expertise' to humans remains a challenge. This project explores a novel approach where human learners are trained using visual feedback derived directly from the activation patterns of a CDNN trained for melanoma identification. The goal is to investigate whether exposing humans to the features that an AI 'sees' can accelerate and enhance their own perceptual learning, creating a synergistic human-AI training paradigm."
    ],
    "analyses": [
      "Design and implementation of a human training paradigm using CDNN activation visualizations.",
      "Evaluation of human diagnostic accuracy and confidence pre- and post-training.",
      "Analysis of how human perceptual strategies change after AI-guided training."
    ],
    "outcomes": [""],
    "interactive_demo_link": "",
    "experiment_demo_link": "",
    "osf_link": ""
  },
  {
    "id": "team-spirit-hh",
    "title": "Team Spirit: Evaluating Group Performance Efficiency",
    "category": "Human-AI Collaboration",
    "summary": "This project investigates the performance and cognitive dynamics of human teams working on a novel, dynamic task. It provides a quantitative framework for measuring team efficiency and understanding the cognitive costs of teamwork in both collaborative and competitive settings.",
    "image": "assets/images/project-icons/team-spirit-welcome.png",
    "background": [
      "It's well-known that teams can accomplish more than individuals. Taking your friend to the supermarket allows you to pick up more items than if you had gone alone, but is it better to work through the shopping list together or split it in two, go about the aisles separately, then meet back at the checkout? While teamwork is often beneficial, it isn't always particularly efficient! (Perhaps a more powerful example is that of a group assignment, but let's not go there!). In this project we develop a reliable and objective measure of team performance efficiency and simultaneously examine the cognitive cost to individuals when they have to coordinate their efforts with a teammate. We designed a unique experiment using a dynamic, online game inspired by the classic 'Pong'. <a href='experiment-demo-viewer.html?demo=team-spirit-hh'>Try it yourself here.</a>"
    ],
    "analyses": [
      "<h4>Playing Space and Task</h4>",
      "Participant pairs played a simple computer game where players moved paddles to deflect incoming balls. The experiment included three group conditions: a <strong>Separate</strong> condition where players worked independently, a <strong>Collaborative</strong> condition where they worked together for a single team score, and a <strong>Competitive</strong> condition where they tried to outscore their opponent. This allowed for the direct comparison of individual performance with both collaborative and competitive teamwork.",
      "<img src='assets/images/team-spirit-hh/team-high-drtOff.png' alt='Playing screen examples in the team group condition' style='width:50%'>",
      "<h4>Workload Capacity Analysis</h4>",
      "Workload capacity is a measure that goes beyond a simple team-score and allows us to evaluate the <em>efficiency</em> of teamwork. By comparing the team performance to the theoretical UCIP benchmark (Unlimited Capacity Independent and Parallel). This benchmark representes what two players would achieve if they worked independently of each other and at the same time (parallel). By comparing observed performance to this theoretical benchmark, we can quantitatively determine if teamwork introduced a cost or benefit to the overall performance. This analysis provides a robust quantitative tool to examine the quality of team processes.",
      "<h4>Detection Response Task (DRT)</h4>",
      "We also implemented a secondary signal detection task that was presented concurrently with the game to assess individual cognitive load. Participants had to respond to a brief flash of light, where slower response times or lower accuracy on this task indicate that a participant is experiencing a higher <em>cognitive load</em> from the demands of the main game and from the act of teamwork itself.",
      "<h4>n-Balls Maintained Transformation</h4>",
      "A novel way to interpret the main game's miss rate data. This analysis converted miss rates into a more intuitive metric: the number of balls a player or team could maintain at a fixed accuracy level. For example, it showed that while individuals could handle about 4 balls, a collaborative team could handle about 7, providing a more tangible measure for comparing performance across different group conditions.",
      "<img src='assets/images/team-spirit-hh/nBalls_maintained.png' alt='A visual representation of the n-balls maintained transformation' style='width:50%'>"
    ],
    "outcomes": [
      "<h4>Teams are More Productive, but Less Efficient</h4>",
      "While both collaborative and competitive teams kept more balls in play than individuals, the workload capacity analysis revealed that teamwork introduced a measurable cost. In the figure below, data points below the gray horizontal line at 0 (the UCIP benchmark) indicate a performance <em>cost</em>. This <emlimited capacity</em> meant that teams were not performing as well as one would expect from two independently working individuals. This indicates that the dynamics of teamwork, regardless of the goal, can be a source of inefficiency.",
      "<img src='assets/images/team-spirit-hh/HH-CapacityMR.png' alt='Workload capacity metrics for collaborative and competitive human teams' style='width:50%'>",
      "<h4>Collaboration is a More Efficient Strategy</h4>",
      "We found that collaborative teams had a significantly smaller performance cost than competitive teams. This suggests that the dynamics of working together towards a common goal are inherently more efficient than the behaviors that emerge from trying to outscore a teammate. This finding has practical implications for designing systems that foster cooperation to optimize performance.",
      "<h4>Teamwork Increases Cognitive Load</h4>",
      "The DRT data showed that participants in both collaborative and competitive groups experienced a higher cognitive load than individuals. This suggests that the mental effort of coordinating with another person, even without direct communication, consumes valuable cognitive resources. However, it’s important to note that the task's high difficulty led to a ceiling effect, meaning this result was primarily evident in the lowest workload condition.",
      "<h4>Conclusion</h4>",
      "This project provides a compelling application of workload capacity analysis for dynamic team environments. It demonstrates the utility of a psychometrically sound approach to quantifying team efficiency and identifying the specific costs of different group conditions. This framework can now be used in future research to examine how other features of an individual or team (such as working with an AI teammate) might influence performance costs."
    ],
    "interactive_demo_link": "",
    "experiment_demo_link": "experiment-demo-viewer.html?demo=team-spirit-hh",
    "osf_link": "https://osf.io/mtazf/"
  },
  {
    "id": "implicit-attitudes",
    "title": "Implicit Attitudes Task (IAT) Processing Independence",
    "category": "Cognitive Modeling & Decision Science",
    "summary": "Empirically testing processing independence in the Implicit Attitudes Task (IAT), a widely used measure in social psychology, to refine our understanding of automatic cognitive processes.",
    "image": "https://placehold.co/600x400/ADD8E6/4682B4?text=Implicit+Attitudes",
    "background": [
      "The Implicit Attitudes Task (IAT) is a cornerstone of implicit social cognition research, designed to measure automatic associations between concepts. A key theoretical assumption underlying the IAT is the independence of different processing components (e.g., target-attribute association vs. response mapping). This project empirically tests this assumption using advanced cognitive modeling techniques and carefully controlled experimental manipulations, contributing to the ongoing debate about the IAT's validity and interpretation."
    ],
    "analyses": [
      "Application of process dissociation procedures to IAT data.",
      "Cognitive modeling (e.g., multinomial processing tree models) to disentangle independent processing components.",
      "Analysis of how experimental manipulations affect different components of IAT performance."
    ],
    "outcomes": [""],
    "interactive_demo_link": "",
    "experiment_demo_link": "",
    "osf_link": ""
  }
]
